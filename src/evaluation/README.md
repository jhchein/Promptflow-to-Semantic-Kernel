[Azure AI Foundry Observability Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/observability)

![Pre-Production Evaluation](https://learn.microsoft.com/en-us/azure/ai-foundry/media/evaluations/evaluation-models-diagram.png)

## Evaluation cheat sheet

| Purpose                                       | Process                                   | Parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| --------------------------------------------- | ----------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| What are you evaluating for?                  | Identify or build relevant evaluators     | \- [Quality and performance sample notebook](https://github.com/Azure-Samples/rag-data-openai-python-promptflow/blob/main/src/evaluation/evaluate.py) <br> <br>\- [Agents Response Quality](https://github.com/Azure-Samples/azureai-samples/tree/main/scenarios/evaluate/Supported_Evaluation_Metrics/Agent_Evaluation) <br> <br>\- [Safety and Security](evaluation-evaluators/risk-safety-evaluators) ([Safety and Security sample notebook](https://github.com/Azure-Samples/rag-data-openai-python-promptflow/blob/main/src/evaluation/evaluatesafetyrisks.py)) <br> <br>\- [Custom](evaluation-evaluators/custom-evaluators) ([Custom sample notebook](https://github.com/Azure-Samples/rag-data-openai-python-promptflow/blob/main/src/evaluation/evaluate.py))             |
| What data should you use?                     | Upload or generate relevant dataset       | [Generic simulator for measuring Quality and Performance](concept-synthetic-data) ([Generic simulator sample notebook](https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/system/finetune/Llama-notebooks/datagen/synthetic-data-generation.ipynb)) <br> <br>\- [Adversarial simulator for measuring Safety and Security](../how-to/develop/simulator-interaction-data) ([Adversarial simulator sample notebook](https://github.com/Azure-Samples/rag-data-openai-python-promptflow/blob/main/src/evaluation/simulate_and_evaluate_online_endpoint.ipynb)) <br> <br>AI red teaming agent for running automated scans to assess safety and security vulnerabilities ([AI red teaming agent sample notebook](https://aka.ms/airedteamingagent-sample)) |
| What resources should conduct the evaluation? | Run evaluation                            | \- [Local run](../how-to/develop/evaluate-sdk) <br> <br>\- [Remote cloud run](../how-to/develop/cloud-evaluation)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| How did my model/app perform?                 | Analyze results                           | [View aggregate scores, view details, score details, compare evaluation runs](../how-to/evaluate-results)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| How can I improve?                            | Make changes to model, app, or evaluators | \- If evaluation results didn't align to human feedback, adjust your evaluator. <br> <br>\- If evaluation results aligned to human feedback but didn't meet quality/safety thresholds, apply targeted mitigations. Example of mitigations to apply: [Azure AI Content Safety](../ai-services/content-safety-overview)                                                                                                                                                                                                                                                                                                                                                                                                                                                              |

## Region support

Currently certain AI-assisted evaluators are available only in the following regions:

| Region           | Hate and unfairness, Sexual, Violent, Self-harm, Indirect attack, Code vulnerabilities, Ungrounded attributes | Groundedness Pro | Protected material |
| ---------------- | ------------------------------------------------------------------------------------------------------------- | ---------------- | ------------------ |
| East US 2        | Supported                                                                                                     | Supported        | Supported          |
| Sweden Central   | Supported                                                                                                     | Supported        | N/A                |
| US North Central | Supported                                                                                                     | N/A              | N/A                |
| France Central   | Supported                                                                                                     | N/A              | N/A                |
| Switzerland West | Supported                                                                                                     | N/A              | N/A                |
